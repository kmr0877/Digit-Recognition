{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-431f149fe51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_placeholder():\n",
    "    \"\"\"\n",
    "    This placeholder serves as the input to the model, and will be populated\n",
    "    with the raw images, flattened into single row vectors of length 784.\n",
    "\n",
    "    The number of images to be stored in the placeholder for each minibatch,\n",
    "    i.e. the minibatch size, may vary during training and testing, so your\n",
    "    placeholder must allow for a varying number of rows.\n",
    "\n",
    "    :return: A tensorflow placeholder of type float32 and correct shape\n",
    "    \"\"\"\n",
    "    return tf.placeholder(dtype=tf.float32, shape=[None, 784],\n",
    "                          name=\"image_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_placeholder():\n",
    "    \"\"\"\n",
    "    This placeholder serves as the output for the model, and will be\n",
    "    populated with targets for training, and testing. Each output will\n",
    "    be a single one-hot row vector, of length equal to the number of\n",
    "    classes to be classified (hint: there's one class for each digit)\n",
    "\n",
    "    The number of target rows to be stored in the placeholder for each\n",
    "    minibatch, i.e. the minibatch size, may vary during training and\n",
    "    testing, so your placeholder must allow for a varying number of\n",
    "    rows.\n",
    "\n",
    "    :return: A tensorflow placeholder of type float32 and correct shape\n",
    "    \"\"\"\n",
    "    return tf.placeholder(dtype=tf.float32, shape=[None, 10],\n",
    "                          name=\"image_target_onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onelayer(X, Y, layersize=10,INPUT_SIZE=784,OUTPUT_SIZE=10):\n",
    "    \"\"\"\n",
    "    Create a Tensorflow model for logistic regression (i.e. single layer NN)\n",
    "\n",
    "    :param X: The  input placeholder for images from the MNIST dataset\n",
    "    :param Y: The output placeholder for image labels\n",
    "    :return: The following variables should be returned  (variables in the\n",
    "    python sense, not in the Tensorflow sense, although some may be\n",
    "    Tensorflow variables). They must be returned in the following order.\n",
    "        w: Connection weights\n",
    "        b: Biases\n",
    "        logits: The input to the activation function\n",
    "        preds: The output of the activation function (a probability\n",
    "        distribution over the 10 digits)\n",
    "        batch_xentropy: The cross-entropy loss for each image in the batch\n",
    "        batch_loss: The average cross-entropy loss of the batch\n",
    "    \"\"\"\n",
    "    with tf.name_scope('layer1'):\n",
    "        w = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_SIZE, layersize],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_SIZE))),\n",
    "            name='weights')\n",
    "        b = tf.Variable(tf.zeros([layersize]),\n",
    "                             name='biases')\n",
    "        hidden = tf.nn.relu(tf.matmul(X, w) + b)\n",
    "\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([layersize, OUTPUT_SIZE],\n",
    "                                stddev=1.0 / math.sqrt(float(layersize))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([OUTPUT_SIZE]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden, weights) + biases\n",
    "\n",
    "    preds=tf.nn.softmax(logits)\n",
    "    batch_xentropy =  tf.nn.softmax_cross_entropy_with_logits(labels=Y,\n",
    "                                                  logits=logits,\n",
    "                                                  name=\"xentropy\")\n",
    "\n",
    "    batch_loss=tf.reduce_mean(batch_xentropy)\n",
    "\n",
    "\n",
    "\n",
    "    return w, b, logits, preds, batch_xentropy, batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twolayer(X, Y, hiddensize1=100, hiddensize2=30,INPUT_SIZE=784,OUTPUT_SIZE=10):\n",
    "    \"\"\"\n",
    "    Create a Tensorflow model for a Neural Network with one hidden layer\n",
    "\n",
    "    :param X: The  input placeholder for images from the MNIST dataset\n",
    "    :param Y: The output placeholder for image labels\n",
    "    :return: The following variables should be returned in the following order.\n",
    "        W1: Connection weights for the first layer\n",
    "        b1: Biases for the first layer\n",
    "        W2: Connection weights for the second layer\n",
    "        b2: Biases for the second layer\n",
    "        logits: The inputs to the activation function\n",
    "        preds: The outputs of the activation function (a probability\n",
    "        distribution over the 10 digits)\n",
    "        batch_xentropy: The cross-entropy loss for each image in the batch\n",
    "        batch_loss: The average cross-entropy loss of the batch\n",
    "    \"\"\"\n",
    "    with tf.name_scope('layer1'):\n",
    "        w1 = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_SIZE, hiddensize1],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_SIZE))),\n",
    "            name='weights')\n",
    "        b1 = tf.Variable(tf.zeros([hiddensize1]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "        # Hidden 2\n",
    "    with tf.name_scope('layer2'):\n",
    "        w2 = tf.Variable(\n",
    "            tf.truncated_normal([hiddensize1, hiddensize2],\n",
    "                                stddev=1.0 / math.sqrt(float(hiddensize1))),\n",
    "            name='weights')\n",
    "        b2 = tf.Variable(tf.zeros([hiddensize2]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, w2) + b2)\n",
    "        # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hiddensize2, OUTPUT_SIZE],\n",
    "                                stddev=1.0 / math.sqrt(float(hiddensize2))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([OUTPUT_SIZE]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    preds=tf.nn.softmax(logits)\n",
    "    batch_xentropy =  tf.nn.softmax_cross_entropy_with_logits(labels=Y,\n",
    "                                                  logits=logits,\n",
    "                                                  name=\"xentropy\")\n",
    "\n",
    "    batch_loss=tf.reduce_mean(batch_xentropy)\n",
    "\n",
    "    return w1, b1, w2, b2, logits, preds, batch_xentropy, batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W,padding):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(X, Y, convlayer_sizes=[10, 10], \\\n",
    "        filter_shape=[3, 3], outputsize=10, hidden_size=100, padding=\"SAME\"):\n",
    "    \"\"\"\n",
    "    Create a Tensorflow model for a Convolutional Neural Network. The network\n",
    "    should be of the following structure:\n",
    "    conv_layer1 -> conv_layer2 -> fully-connected -> output\n",
    "\n",
    "    :param X: The  input placeholder for images from the MNIST dataset\n",
    "    :param Y: The output placeholder for image labels\n",
    "    :return: The following variables should be returned in the following order.\n",
    "        conv1: A convolutional layer of convlayer_sizes[0] filters of shape filter_shape\n",
    "        conv2: A convolutional layer of convlayer_sizes[1] filters of shape filter_shape\n",
    "        w: Connection weights for final layer\n",
    "        b: biases for final layer\n",
    "        logits: The inputs to the activation function\n",
    "        preds: The outputs of the activation function (a probability\n",
    "        distribution over the 10 digits)\n",
    "        batch_xentropy: The cross-entropy loss for each image in the batch\n",
    "        batch_loss: The average cross-entropy loss of the batch\n",
    "\n",
    "    hints:\n",
    "    1) consider tf.layer.conv2d\n",
    "    2) the final layer is very similar to the onelayer network. Only the input\n",
    "    will be from the conv2 layer. If you reshape the conv2 output using tf.reshape,\n",
    "    you should be able to call onelayer() to get the final layer of your network\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([filter_shape[0],filter_shape[1], 1, convlayer_sizes[0]])\n",
    "        b_conv1 = bias_variable([convlayer_sizes[0]])\n",
    "        conv1 = tf.nn.relu(conv2d(x_image, W_conv1,padding) + b_conv1)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([filter_shape[0],filter_shape[1], convlayer_sizes[0], convlayer_sizes[1]])\n",
    "        b_conv2 = bias_variable([convlayer_sizes[1]])\n",
    "        conv2 = tf.nn.relu(conv2d(conv1, W_conv2,padding) + b_conv2)\n",
    "\n",
    "    conv_height = math.ceil(math.ceil(float(28) / 2.0)/2.0)\n",
    "    conv_width = math.ceil(math.ceil(float(28) / 2.0) /2.0)#stride=2\n",
    "    num_conf_features=conv_width * conv_height * convlayer_sizes[1]\n",
    "    with tf.name_scope('layer1'):\n",
    "        w = tf.Variable(tf.truncated_normal([num_conf_features, hidden_size],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden_size))), name='weights')\n",
    "        b = tf.Variable(tf.zeros([hidden_size]),\n",
    "                             name='biases')\n",
    "        hidden = tf.nn.relu(tf.matmul(tf.reshape(conv2, [-1,num_conf_features]), w) + b)\n",
    "        # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden_size, outputsize],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden_size))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([outputsize]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden, weights) + biases\n",
    "\n",
    "    preds=tf.nn.softmax(logits)\n",
    "    batch_xentropy =  tf.nn.softmax_cross_entropy_with_logits(labels=Y,\n",
    "                                                  logits=logits,\n",
    "                                                  name=\"xentropy\")\n",
    "\n",
    "    batch_loss=tf.reduce_mean(batch_xentropy)\n",
    "\n",
    "    return conv1, conv2, w, b, logits, preds, batch_xentropy, batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(sess, batch, X, Y, train_op, loss_op, summaries_op):\n",
    "    \"\"\"\n",
    "    Run one step of training.\n",
    "\n",
    "    :param sess: the current session\n",
    "    :param batch: holds the inputs and target outputs for the current minibatch\n",
    "    batch[0] - array of shape [minibatch_size, 784] with each row holding the\n",
    "    input images\n",
    "    batch[1] - array of shape [minibatch_size, 10] with each row holding the\n",
    "    one-hot encoded targets\n",
    "    :param X: the input placeholder\n",
    "    :param Y: the output target placeholder\n",
    "    :param train_op: the tensorflow operation that will run one step of training\n",
    "    :param loss_op: the tensorflow operation that will return the loss of your\n",
    "    model on the batch input/output\n",
    "\n",
    "    :return: a 3-tuple: train_op_result, loss, summary\n",
    "    which are the results of running the train_op, loss_op and summaries_op\n",
    "    respectively.\n",
    "    \"\"\"\n",
    "    train_result, loss, summary = \\\n",
    "        sess.run([train_op, loss_op, summaries_op], feed_dict={X: batch[0], Y: batch[1]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
